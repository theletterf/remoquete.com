---
title: Siri, de lo que no se puede hablar hay que callar
author: Fabrizio
type: post
date: 2016-03-21T15:31:11+00:00
url: /2016/03/siri-de-lo-que-no-se-puede-hablar-hay-que-callar/
featured_image: /wp-content/uploads/2016/03/21-03-2016-16-04-16.png
categories:
  - Remoquetes

---
Nos encanta <a href="http://remoquete.com/2014/12/por-que-estudio-lenguajes-de-programacion/" target="_blank">hablar con las máquinas</a>. Parecen tener una respuesta para todo. Están disponibles a todas horas. No nos juzgan y no pueden competir con nosotros. Al compararlas con un chamán o un terapeuta, ganan de calle, pues sabemos que están diseñadas para dar sin pedir nada a cambio. Son dioses menores en una caja.

<a href="http://www.alicebot.org/articles/wallace/eliza.html" target="_blank">Empezó todo con ELISA</a>, la IA que simulaba ser un psiquiatra. El bot que Creative Labs distribuyó con sus tarjetas de sonido, <a href="https://en.wikipedia.org/wiki/Dr._Sbaitso" target="_blank">Dr. Sbaitso</a>, te invitaba a hablar de tus problemas. En <a href="https://es.wikipedia.org/wiki/P%C3%B3rtico_%28novela%29" target="_blank">Pórtico</a>, el protagonista se explaya con un robopsiquiatra. En <a href="https://www.google.es/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjov8yOhtLLAhWCPhQKHdCcBagQFggdMAA&url=http%3A%2F%2Fwww.imdb.com%2Ftitle%2Ftt0066434%2F&usg=AFQjCNGNHaR990KLV-jZEJ4-JPTTs2K-gg&sig2=NaqddFBdEiPLxReq6imYqQ" target="_blank">THX-1138</a>, los ciudadanos se confiesan ante una IA. Incluso hay robots que reconfortan sin hablar, como <a href="http://www.parorobots.com/" target="_blank">PARO</a>.

<img class="aligncenter size-full wp-image-244780717" src="https://i1.wp.com/remoquete.com/wp-content/uploads/2016/03/1.gif?resize=640%2C233" alt="DR. SBAITSO" width="640" height="233" data-recalc-dims="1" /> 

<p style="text-align: center;">
  <em>Distribuido como una broma, Dr. Sbaitso sigue en la memoria de millones de personas</em>
</p>

Por razones que deberíamos preguntar a un antropólogo, a menudo nos resulta más fácil buscar comprensión en las máquinas. Pero eso no significa que hagan un buen trabajo. <a href="http://archinte.jamanetwork.com/article.aspx?articleid=2500043" target="_blank">Un artículo</a> del _Journal of the American Medical Association_ encontró que Siri, Google Now, Cortana y S Voice contestan mal a peticiones de ayuda médica o por violencia.

<img class="aligncenter size-full wp-image-244780713" src="https://i1.wp.com/remoquete.com/wp-content/uploads/2016/03/concerns.png?resize=579%2C980" alt="concerns" width="579" height="980" srcset="https://i1.wp.com/remoquete.com/wp-content/uploads/2016/03/concerns.png?w=579 579w, https://i1.wp.com/remoquete.com/wp-content/uploads/2016/03/concerns.png?resize=177%2C300 177w, https://i1.wp.com/remoquete.com/wp-content/uploads/2016/03/concerns.png?resize=500%2C846 500w, https://i1.wp.com/remoquete.com/wp-content/uploads/2016/03/concerns.png?resize=150%2C254 150w, https://i1.wp.com/remoquete.com/wp-content/uploads/2016/03/concerns.png?resize=400%2C677 400w, https://i1.wp.com/remoquete.com/wp-content/uploads/2016/03/concerns.png?resize=200%2C339 200w" sizes="(max-width: 579px) 100vw, 579px" data-recalc-dims="1" /> 

<p style="text-align: center;">
  <em>Las respuestas de los asistentes virtuales a peticiones médicas (<a href="http://archinte.jamanetwork.com/article.aspx?articleid=2500043" target="_blank">fuente</a>)</em>
</p>

Con razón la estratega de contenidos Sarah Wachter-Boettcher lanzó <a href="https://medium.com/@sara_ann_marie/dear-tech-you-suck-at-delight-86382d101575#.1r3yubff5" target="_blank">un ataque enfurecido</a> a Apple y compañía, que mantuvo incluso después de que Apple <a href="https://medium.com/@sara_ann_marie/update-siri-now-sends-rape-queries-to-rainn-1fd110c0801b#.nk5uqbxha" target="_blank">parcheara</a> aprisa y corriendo a Siri. Las respuestas de los asistentes virtuales a peticiones de auxilio serias reciben respuestas pobres, desesperantes o insultantes.

En otras palabras, el smartphone, que se ha vuelto la primera línea de escucha, que se ha vendido como un dispensador de ayuda portátil, falla terriblemente cuando las peticiones se vuelven serias. Apple, Google y Microsoft no parecen pensar que cada vez más personas enfermas, vulnerables o que viven solas usan su tecnología.

> <p id="c168" class="graf--p graf-after--p">
>   <em>I just want it to stop writing off queries as “not a problem.”</em>
> </p>
> 
> <p id="ea65" class="graf--p graf-after--p">
>   <em>I just want it to do what it’s designed to do: look things up for users, rather than say it doesn’t understand and leave them with a dead end. </em>
> </p>
> 
> <p class="graf--p graf-after--p" style="text-align: right;">
>   <em>&#8211;Sarah Wachter-Boettcher</em>
> </p>

Que esto haya ocurrido no creo que se deba a la priorización a la hora de diseñar un producto, sino al profundo ombliguismo de ingenieros que piensan que sus vidas de suburbios son las de todos, que la tragedia no forma parte de la vida de una persona, y que las emociones negativas no son compatibles con un producto que usamos las 24 horas.

¿Cómo se resuelve esto? Podríamos empezar por aumentar la diversidad de perfiles en los equipos que desarrollan <a href="http://remoquete.com/2016/02/y-la-app-se-hizo-verbo/" target="_blank">apps conversacionales</a>, por ejemplo. Antropólogos, sociólogos, psicólogos, médicos de atención primaria, supervivientes, pacientes&#8230; Todos ellos tienen cabida a la hora de diseñar un producto para personas _reales_ (que no abstractas).

Hasta entonces, hasta que las empresas de tecnología aborden problemas humanos desde un enfoque humanista, lo mejor que las inteligencias artificiales pueden hacer es _callar_, como recomendó Wittgenstein al final de su _Tractatus_, frase que titula este artículo y que vuelvo a repetir aquí: Siri, de lo que no se puede hablar hay que callar.